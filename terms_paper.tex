\documentclass{sigchi}

% Use this section to set the ACM copyright statement (e.g. for
% preprints).  Consult the conference website for the camera-ready
% copyright statement.

% Copyright
\CopyrightYear{2017}
%\setcopyright{acmcopyright}
\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
% DOI
%\doi{http://dx.doi.org/10.475/123_4}
% ISBN
%\isbn{123-4567-24-567/08/06}
%Conference
%\conferenceinfo{CHI'16,}{May 07--12, 2016, San Jose, CA, USA}
%Price
%\acmPrice{\$15.00}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP --
%% Please note you need to make sure the copy for your specific
%% license is used here!
% \toappear{
% Permission to make digital or hard copies of all or part of this work
% for personal or classroom use is granted without fee provided that
% copies are not made or distributed for profit or commercial advantage
% and that copies bear this notice and the full citation on the first
% page. Copyrights for components of this work owned by others than ACM
% must be honored. Abstracting with credit is permitted. To copy
% otherwise, or republish, to post on servers or to redistribute to
% lists, requires prior specific permission and/or a fee. Request
% permissions from \href{mailto:Permissions@acm.org}{Permissions@acm.org}. \\
% \emph{CHI '16},  May 07--12, 2016, San Jose, CA, USA \\
% ACM xxx-x-xxxx-xxxx-x/xx/xx\ldots \$15.00 \\
% DOI: \url{http://dx.doi.org/xx.xxxx/xxxxxxx.xxxxxxx}
% }

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}       % to better equalize the last page
\usepackage{graphics}      % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}   % for umlauts and other diaeresis
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdflang={en-US},pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{csquotes}

% Some optional stuff you might like/need.
\usepackage{microtype}        % Improved Tracking and Kerning
% \usepackage[all]{hypcap}    % Fixes bug in hyperref caption linking
\usepackage{ccicons}          % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of
% your draft document, you have to enable the "chi_draft" option for
% the document class. To do this, change the very first line to:
% "\documentclass[chi_draft]{sigchi}". You can then place todo notes
% by using the "\todo{...}"  command. Make sure to disable the draft
% option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Toxicity Online: Conceptual Devices for Understanding and Explaining Cyberbullying, Online Harassment, and Cyber Aggression}
\def\plainauthor{First Author, Second Author, Third Author,
  Fourth Author, Fifth Author, Sixth Author}
\def\emptyauthor{}
\def\plainkeywords{Authors' choice; of terms; separated; by
  semicolons; include commas, within terms only; required.}
\def\plaingeneralterms{Documentation, Standardization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  pdfdisplaydoctitle=true, % For Accessibility
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
  hypertexnames=false
}

\makeatletter
\def\@copyrightspace{\relax}
\makeatother

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{3}
\author{%
  \alignauthor{Leave Authors Anonymous\\
    \affaddr{for Submission}\\
    \affaddr{City, Country}\\
    \email{e-mail address}}\\
  \alignauthor{Leave Authors Anonymous\\
    \affaddr{for Submission}\\
    \affaddr{City, Country}\\
    \email{e-mail address}}\\
  \alignauthor{Leave Authors Anonymous\\
    \affaddr{for Submission}\\
    \affaddr{City, Country}\\
    \email{e-mail address}}\\
}

\maketitle

\begin{abstract}
	This paper reviews various terms used by Internet researchers: ``cyberbullying'', ``online harassment'', ``cyber aggression'', and ``toxicity''. We examine how scholars use them in both overlapping and disparate ways to refer to myriad hurtful and antisocial online behaviors. These behaviors can have dire and long-lasting effects on victims and online communities and demand researchers' attention. However, without clarity, the terms operate as jargon rather than as devices that help scholars from multiple fields communicate with one another and with the public. We recognize that one problem for the terms is the translation from offline to online spaces, and discuss the inadequacies of the analogies between online and offline behaviors that fall under them. We also illustrate the problems this lack of specificity presents using example incidents from Instagram and Twitter. We then propose a new taxonomy of toxicity as a useful perspective for dealing with these behaviors. Our goal is to clarify terms to provide conceptual tools for scholars to theorize about, model, and reduce toxic Internet behaviors.
\end{abstract}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous} \category{See
  \url{http://acm.org/about/class/1998/} for the full list of ACM
  classifiers. This section is required.}{}{}

\keywords{\plainkeywords}

\section{Introduction}

In recent years, toxic online behaviors have garnered increased attention from the media and have earned a permanent place in public discourse.  In response to what is clearly a widespread problem \cite{Duggan2014Online} with potentially dire consequences \cite{Dean2012Story,Cohen2015Transgender}, computer-mediated communication (CMC) and computer science (CS) researchers have begun working toward technological interventions, often focusing on machine-learning approaches \cite{Sood2012Automatic,Chen2012Detecting,Dadvar2012Towards,Dinakar2011Modeling,Reynolds2011Using}.  While many people across a multitude of fields in both research and industry are working toward curbing this problem, the precise nature of the problem remains unclear.  Laypeople and reserachrs alike appear to hold a multitude of beliefs about what is and is not harassment, cyber bullying, or hurtful behavior, and whether harassment and cyber bullying are unique categories.  Pater and colleagues \cite{Pater2016Characterizations} describe how, in industry, this lack of consensus is exemplified by the great differences between social-media platfroms in what is and is not considered appropriate behavior .  Among the general public, the myriad of opinions is typified by the difference between what groups (e.g. teenagers and adults) consider to be cyber bullying \cite{Boyd2014Bullying} as differnces of opinion within groups (e.g. parents) \cite{Davis2015Parents}.

It is probably fair to say that most of us have, in broad terms, a good idea of of what is or is not online harassment, cyber bullying, and/or cyber-aggression.  But, what would you say if asked to provide detailed definitions for these terms?  Being able to differentiate between malicious and benign online content depends not only on our own sense of where the lines are, but also on agreeing with eachother about when these lines are crossed.  The most common method of training machine-learning classifiers to detect inappropriate online behavior relies on people like us to manually differentiate malicious content from benign content.  The reliability of machine-learning classifiers is tied to the inter-rater reliability of the humans who provide the training data.  Guberman and Hemphil \cite{Guberman2017Challenges} found that, even when provided with quite specific definitions for labelling content, human raters still differed in their interpretations for a variety reasons.  

In this paper, we focus primarily on the varied definitions of the terms, as used by internet researchers.  We highlight the usage of three main terms: ``online harassment'', ``cyber bullying'', and ``cyber aggression''.  Beyond the methodological implications of the strenght of our definitions, it is important to the research community as a whole that we understand what our peers are referring to when they use these terms.  Often, these terms used without operational definitons, such that it is unclear whether researchers are talking about the same phenomena.  In other cases, one of these terms will be used as an umbrella term encompassing the other two terms.  There seems to be disagreement about which of these terms is the superset under which the other terms fit.  Additionally, some researchers go to great lengths to justify the adaptation of traditional definitons of bullying to the various antisocial behaviors observed online, even when evidence suggests that these adaptated definitions do not adequately describe the online phenomena.  We believe it is necessery to reconcile these differing perspectives and definitons in order to combat the problem most effectively.  As a community, it is important that we agree about the nature of this problem, and that our definitions conform not only to the observed phenomena, but also to the types of behavior members of the public / users of various CMC platforms actaully find to be concerning.  After a discussion of the existing usages of these terms, we propose a new taxonomy in which the superset is less value-laden than terms like cyber bullying.  We suggest researchers focus on specific behaviors, which may vary in severity and required action depending on the contexts in which they occur.

\section{Online Harassment}

While reserach into human issues in computing, such as online harssment, are currently in vogue (and for good reason), academics have been investigating harassment in online spaces since the nascent era of modern CMC tools.  In the later years of the internet-relay chat's popularity (IRC) , Herring \cite{Herring1999Rhetorical} investigated the ways in which gender-based harassment manifested online.  Herring operationalizes harassment using a definiton from \textit{Black's Law Dictionary} as behaviors (either singularly or in repetition) ```which tended to annoy, alarm and verbally abuse''' individuals [p. 151-152].  Operationalized as such, Herring uses ``harassment'' to refer to a variety of behaviors falling under the umbrella of the definition provided.  Among these behaviors are attempts to provoke, intimidate, and silence female IRC users.  

Using ``online harassment'' as an umbrella category under which nearly all concivable negative online behaviors fit is a common occurence in the literature.  Unlike Herring \cite{Herring1999Rhetorical}, however, who provides a clear definition for the operationalization of ``online harassment'' as a superset, others appear to define the superset by the behaviors it encompasses (or, viewed another way, do not define the superset at all).  For example, in the Pew Research Center's 2014 report no online harrassment, Duggan does not explicityly define harassment itself \cite{Duggan2014Online}.  Instead, Duggan refers to harassment as a spectrum that contains a range of (presumably negative) behaviors ``from garden-variety name calling to more threatening behavior'' [p. 1].  The specific harassing behaviors referred to in the report include offensive name-calling, physical threats, sustained harassment over a period of time, sexual harassment, and stalking.  Reiterating the notian that these behaviors exist on a continuum of severity, Duggan explains that men endure more harrasment than women, but women are disproportianately more likely to be victimized by the more severe types of harassment.

Like Duggan \cite{Duggan2014Online}, Lenhart and colleagues \cite{Lenhart2016Online} also operationalize ``online harassment'' as both a superset and as a continuum.  They provide one of the most explicitly inclusive definitions of online harrassment that we have seen:

\begin{displayquote}
Harassment can encompass a wide range of unwanted contact that is used to create an intimidating, annoying, frightening, or even hostile environment for the victim. Online harassment is generally recognized as referring to this type of negative and unwanted contact using digital means. Online harassment can be a brief occurrence or a sustained campaign of abuse and attacks; the perpetrator (or perpetrators) might be intimately known to the victim, or a stranger in another state or country. Online harassment is defined less by the specific behavior than its intended effect on and the way it is experienced by its target.
\end{displayquote}

They note that their operationalization differs from legal definitions, which require harrassment to be methodical and/or repetitive in nature.  We would like to draw attention to the distinction at the end of their definition; to Lenhart and colleagues, whether or not a behavior constitutes online harassment is linked to the \textit{inent} behind the behavior.  By this rationale, it is unclear how one would classify behaviors by individuals who intend no harm to others, but which cause harm nonetheless.  Turkle notes that, perhaps due to a stark decrease in empathy over several decades, there are instances in which adolescents may hurt their friends over CMC platforms with no recognition or understanding of how their behaviors were cruel \cite{Turkle2015Reclaiming}.  

In addition to providing several fairly specific harrassing behaviors (n=20, including the six behaviors reported on by \cite{Duggan2014Online}), Lenhart and colleagues \cite{Lenhart2016Online} create a slightly more complex taxonomy by including three intermediate categories.  Their 20 specifit types of harassment fit into the broad cateogories of \textit{direct harassment}, \textit{invasion of privacy}, and \textit{denial of access}.  While these three categories seem useful for discussing harassment, it remains to be seen whether they will have utility for machine-learning approaches to stemming the behaviors contained within.

Lenhart and colleagues' \cite{Lenhart2016Online} definition of harassment is clearly stated and comprises specific behaviors which can be grouped together into convenient categories.  However, it also includes some rather ambigous language.  For example, early in the paper, Lenhart and colleagues define harassment and abuse as both consisting of ``unwanted contact that is used to create an intimidating, annoying, frightening, or even hostile environment for the fictim and that uses digital means to reach the target'' [p. 3].  Being that the two terms appear to be synonymous, their use in the report's title, \textit{Online Harassment, Areigital Abuse, and Cyberstalking in America} is confusing.  The title seems to indicate that online harassment, digital abuse, and cyberstalking are three different, but related, things.  Throughout the paper, the authors refer to both ``harassment and abuse'' and ``harassment or abuse,'' such that it is further unclear whether they intend for these terms to be used interchangably, or whether there is some subtle, yet unstated, difference.  Additionally unclear is the rationale for the elevation of \textit{cyberstalking} to a seperate entry in the paper's title, as it is referred to as a type of harassment (not something distinct from harassment and/or abuse) throughout the paper itself \footnotemark[1].  The comprehensiveness of Lenhart and colleagues' reported results, which are quite elucidating and build upon those of Duggan \cite{Duggan2014Online}, is tampered by the ambiguity injected by the loose usage of these terms.

\footnotetext[1]{In addition to being used in a somewhat confusing manner, there is a question of whether or not \textit{cyberstalking}, a which seems to imply something set apart from traditional stalking, is a useful term.  Some research shows that cyberstalking is much more of an outgrowth of traditional stalking, rather than a seperate entity , and referring to it as something seperate may obfuscate the severity of incidents by directing attention away from whatever stalkign may simultaneously be hapenning to a victim offline \cite{Sheridan2007Is}.}

In contrast to Duggan \cite{Duggan2014Online}, Lenhart and collagues \cite{Lenhart2016Online}, and Lenhart \cite{Lenhart2010Cyberbullying} who consider cyberbullying to be a type of online harrasment, as well as in contrast to Lenhart \cite{Lenhart2007Cyberbullying} and Jones and colleagues \cite{Jones2013Online} who use the two terms interchangeably, Wolak and colleagues \cite{Wolak2007Does} ask whehter online harassment is a manifestation of cyberbullying.  That is, whereas online harassment is typically operationalized as the superset into which cyberbullying fits, Wolak and colleagues wonder if the reverse might be true.  We will delve into their results and into their usage of the term ``cyberbullying'' later.  They defined online harassment as any threats or offensive behaviors, non-inclusive of sexual solicitation, sent to or posted publicly about a child or adolescent.  Implied in this definiton, although not as clearly made explicit as in the previous examples, is a taxonomy of behaviors in which ``harassment'' is the umbrella term.  This operationalization is quite inclusive, albeit not as clear as that of Lenhart and colleagues \cite{Lenhart2016Online}, and only inclusive of behaviors targeted towards minors.  While the paper and its venue of publication are clearly focused on harassment as it pertains to minors, this particular definition ignores the fact that adults are also quite likely to be victims of online harassment \cite{Duggan2014Online,Lenhart2016Online}, making the definition potentially misleading.

The examples discussed above are indicative of the ways in which researchers of the nature and reach of hurtful online behaviors talk about these phenomena.  Online harassment is generally a superset under which a myriad of behaviors fall.  Sometimes the behaviors are clustered into groups, and sometimes they're ranked by severity.  As researchers focus in on ever more specific types of harassment, definitions become more precise (and arguably, useful for discussing the behaviors themselves).  Just as Pater has found that different social-media platforms consider different behaviors to count as harassment \cite{Pater2016Characterizations}, so, too, do researchers.  Some researchers focus on singular, specific behaviors that comprise harassment \cite{Moor2010Flaming}.  There is, perhaps, a need for more of such papers to tell us about the more granular details of what online harassment looks like.  Interestingly, we found only a few instances in which self-harm was categorized as harassing behavior.  One of these instances was Pater \cite{Pater2016Characterizations}, who included self-harm in her investigation of social media platforms' harassment policies.  The other was Boyd \cite{Boyd2014Bullying}, who noted that some degree of online harassment is perpetrated by adolescents against themselves, perhaps for attention or validation.

While research geared toward understanding harassment uses the vocabulary described above, research geared toward technical solutions to the problem of harassment has a different lexicon.  These studies often use terms other than online harassment.  When online harassment \textit{is} used, it is rarely operationalized at all.  Instead, it refers to a set of features of online content presumed to be undesirable.  For example, Dadvar and Jong \cite{Dadvar2012Cyberbullying} talk only about detecting ``harrassing sentences.''  What are harassing sentences?  In this case, a sentence is harassing depending on whehter it contains profanity, the pronouns employed, and the gender of the person that posted the content.  They use cyberbullying interchangeably.  This approach, in contrast to the definitions described earlier, is highly exclusionary and likely to lead to a lot of mis-labeling of content \cite{Guberman2017Challenges}.  In another early case of applying machine-learning to online harassment, Yin and colleagues \cite{Yin2009Detection} first acknowledge the ambiguity surrounding the term ``online harassment'' before defining it as any intentional action meant to annoy another user in the community.  Beyond that, they zero-in on a specific class of harassing behavior ``in which a user systematically deprecates the contributions of another user'' [p. 2].  This definition is inclusive, and the specific behavior is on which Herring \cite{Herring1999Rhetorical} also identified.  While the definition they use is reasonable, the way it is operationalized in their classifiers is potentially problematic.  While they include contextual and sentiment features, the reliance on profanity is an unreliable indicator of harassment (as they themselves reported).  

\section{Cyber Bullying}

There appears to be a good deal of overlap between the ways in which researchers employ the terms ``online harassment'' and ``cyberbullying.''  In fact, there are researchers who create the exact types of taxonomies described in the previous section referring to the superset as ``cyberbullying'' rather than as ``online harassment'' (or use the two terms completely interchangably) \cite{Lenhart2007Cyberbullying}.  More frequently, however, at least in the space in which researchers are focusing on the nature, reach, and impact of cyberbullying, definitions of the term tend to be less inclusive than those used in similar situations to describe online harassment.  The definitons of cyberbullying are more numerous and more varied.  In one case, in introducing the idea of cyberbullying, a single paper draws upon more than five seperate definitions.  They consider bullying to be the use of ICTS to maliciously and repeatedly threaten people, threats sent via ICTs that cause psychological and social problems for victims, a type of psychological bullying occuring via ICTs, a form of social aggression, \textit{and} an extension of traditional bullying with several notable exceptions \cite{Cetin2011Cyber}.  Unfortunately, not all of the definitions the authors combined are compatable with one another.  In this section, we will discuss some of the myriad definitions of cyberbullying, definitions of cyberbullying based on traditional bullying and the problems thereof, and the ways in which different research communities are talking about cyberbullying in quite different ways.

Among those trying to figure out just what cyberbullying use, not including the definitions that are synonymous with the taxonomies of online harassment, two types definitions of cyberbullying prevail.



\section{Cyber Aggression}

\section{Toxicity}

\section{Moving Forward}

% BALANCE COLUMNS
\balance{}

% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{Remote.bib}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
