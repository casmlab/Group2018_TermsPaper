\documentclass{sigchi}

% Use this section to set the ACM copyright statement (e.g. for
% preprints).  Consult the conference website for the camera-ready
% copyright statement.

% Copyright
\CopyrightYear{2017}
%\setcopyright{acmcopyright}
\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
% DOI
%\doi{http://dx.doi.org/10.475/123_4}
% ISBN
%\isbn{123-4567-24-567/08/06}
%Conference
%\conferenceinfo{CHI'16,}{May 07--12, 2016, San Jose, CA, USA}
%Price
% * <xrao@hawk.iit.edu> 2017-06-27T15:51:09.913Z:
%
% > %\conferenceinfo{CHI'16,}{May 07--12, 2016, San Jose, CA, USA}
%
% ^.
%\acmPrice{\$15.00}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP --
%% Please note you need to make sure the copy for your specific
%% license is used here!
% \toappear{
% Permission to make digital or hard copies of all or part of this work
% for personal or classroom use is granted without fee provided that
% copies are not made or distributed for profit or commercial advantage
% and that copies bear this notice and the full citation on the first
% page. Copyrights for components of this work owned by others than ACM
% must be honored. Abstracting with credit is permitted. To copy
% otherwise, or republish, to post on servers or to redistribute to
% lists, requires prior specific permission and/or a fee. Request
% permissions from \href{mailto:Permissions@acm.org}{Permissions@acm.org}. \\
% \emph{CHI '16},  May 07--12, 2016, San Jose, CA, USA \\
% ACM xxx-x-xxxx-xxxx-x/xx/xx\ldots \$15.00 \\
% DOI: \url{http://dx.doi.org/xx.xxxx/xxxxxxx.xxxxxxx}
% }

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}       % to better equalize the last page
\usepackage{graphics}      % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}   % for umlauts and other diaeresis
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdflang={en-US},pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{csquotes}

% Some optional stuff you might like/need.
\usepackage{microtype}        % Improved Tracking and Kerning
% \usepackage[all]{hypcap}    % Fixes bug in hyperref caption linking
\usepackage{ccicons}          % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of
% your draft document, you have to enable the "chi_draft" option for
% the document class. To do this, change the very first line to:
% "\documentclass[chi_draft]{sigchi}". You can then place todo notes
% by using the "\todo{...}"  command. Make sure to disable the draft
% option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Toxicity Online: Conceptual Devices for Understanding and Explaining Cyberbullying, Online Harassment, and Cyber Aggression}
\def\plainauthor{First Author, Second Author, Third Author,
  Fourth Author, Fifth Author, Sixth Author}
\def\emptyauthor{}
\def\plainkeywords{Authors' choice; of terms; separated; by
  semicolons; include commas, within terms only; required.}
\def\plaingeneralterms{Documentation, Standardization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  pdfdisplaydoctitle=true, % For Accessibility
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
  hypertexnames=false
}

\makeatletter
\def\@copyrightspace{\relax}
\makeatother

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{3}
\author{%
  \alignauthor{Leave Authors Anonymous\\
    \affaddr{for Submission}\\
    \affaddr{City, Country}\\
    \email{e-mail address}}\\
  \alignauthor{Leave Authors Anonymous\\
    \affaddr{for Submission}\\
    \affaddr{City, Country}\\
    \email{e-mail address}}\\
  \alignauthor{Leave Authors Anonymous\\
    \affaddr{for Submission}\\
    \affaddr{City, Country}\\
    \email{e-mail address}}\\
}

\maketitle

\begin{abstract}
	This paper reviews various terms used by Internet researchers: ``cyberbullying'', ``online harassment'', ``cyber aggression'', and ``toxicity''. We examine how scholars use them in both overlapping and disparate ways to refer to myriad hurtful and antisocial online behaviors. These behaviors can have dire and long-lasting effects on victims and online communities and demand researchers' attention. However, without clarity, the terms operate as jargon rather than as devices that help scholars from multiple fields communicate with one another and with the public. We recognize that one problem for the terms is the translation from offline to online spaces, and discuss the inadequacies of the analogies between online and offline behaviors that fall under them. We also illustrate the problems this lack of specificity presents using example incidents from Instagram and Twitter. We then propose a new taxonomy of toxicity as a useful perspective for dealing with these behaviors. Our goal is to clarify terms to provide conceptual tools for scholars to theorize about, model, and reduce toxic Internet behaviors.
\end{abstract}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous} \category{See
  \url{http://acm.org/about/class/1998/} for the full list of ACM
  classifiers. This section is required.}{}{}

\keywords{\plainkeywords}

\section{Introduction}

In recent years, toxic online behaviors have garnered increased attention from the media and have earned a permanent place in public discourse.  In response to what is clearly a widespread problem \cite{Duggan2014Online} with potentially dire consequences \cite{Dean2012Story,Cohen2015Transgender}, computer-mediated communication (CMC) and computer science (CS) researchers have begun working toward technological interventions, often focusing on machine-learning approaches \cite{Sood2012Automatic,Chen2012Detecting,Dadvar2012Towards,Dinakar2011Modeling,Reynolds2011Using}.  While many people across a multitude of fields in both research and industry are working toward curbing this problem, the precise nature of the problem remains unclear.  Laypeople and research alike appear to hold a multitude of beliefs about what is and is not harassment, cyber bullying, or hurtful behavior, and whether harassment and cyber bullying are unique categories.  Pater and colleagues \cite{Pater2016Characterizations} describe how, in industry, this lack of consensus is exemplified by the great differences between social-media platforms in what is and is not considered appropriate behavior .  Among the general public, the myriad of opinions is typified by the difference between what groups (e.g. teenagers and adults) consider to be cyber bullying \cite{Boyd2014Bullying} as well as by differences of opinion within groups (e.g. parents) \cite{Davis2015Parents}.

It is probably fair to say that most of us have, in broad terms, a good idea of of what is or is not online harassment, cyber bullying, and/or cyber-aggression.  But, what would you say if asked to provide detailed definitions for these terms?  Being able to differentiate between malicious and benign online content depends not only on our own sense of where the lines are, but also on agreeing with each other about when these lines are crossed.  The most common method of training machine-learning classifiers to detect inappropriate online behavior relies on people like us to manually differentiate malicious content from benign content.  The reliability of machine-learning classifiers is tied to the inter-rater reliability of the humans who provide the training data.  Guberman and Hemphill \cite{Guberman2017Challenges} found that, even when provided with quite specific definitions for labeling content, human raters still differed in their interpretations for a variety reasons.  

In this paper, we focus primarily on the varied definitions of the terms, as used by internet researchers.  We highlight the usage of three main terms: ``online harassment'', ``cyber bullying'', and ``cyber aggression''.  Beyond the methodological implications of the strength of our definitions, it is important to the research community as a whole that we understand what our peers are referring to when they use these terms.  Often, these terms used without operational definitions, such that it is unclear whether researchers are talking about the same phenomena.  In other cases, one of these terms will be used as an umbrella term encompassing the other two terms.  There seems to be disagreement about which of these terms is the superset under which the other terms fit.  Additionally, some researchers go to great lengths to justify the adaptation of traditional definitions of bullying to the various antisocial behaviors observed online, even when evidence suggests that these adapted definitions do not adequately describe the online phenomena.  We believe it is necessary to reconcile these differing perspectives and definitions in order to combat the problem most effectively.  As a community, it is important that we agree about the nature of this problem, and that our definitions conform not only to the observed phenomena, but also to the types of behavior members of the public / users of various CMC platforms actually find to be concerning.  After a discussion of the existing usages of these terms, we propose a new taxonomy in which the superset is less value-laden than terms like cyber bullying.  We suggest researchers focus on specific behaviors, which may vary in severity and required action depending on the contexts in which they occur.

\section{Online Harassment}

While research into human issues in computing, such as online harassment, are currently in vogue (and for good reason), academics have been investigating harassment in online spaces since the nascent era of modern CMC tools.  In the later years of the internet-relay chat's popularity (IRC), Herring \cite{Herring1999Rhetorical} investigated the ways in which gender-based harassment manifests online.  Herring operationalizes harassment using a definition from \textit{Black's Law Dictionary} as behaviors (either singularly or in repetition) ```which tended to annoy, alarm and verbally abuse''' individuals [p. 151-152].  Operationalized as such, Herring uses ``harassment'' to refer to a variety of behaviors falling under the umbrella of the definition provided.  Among these behaviors are attempts to provoke, intimidate, and silence female IRC users.  

Using ``online harassment'' as an umbrella category under which nearly all conceivable negative online behaviors fit is a common occurrence in the literature.  Unlike Herring \cite{Herring1999Rhetorical}, however, who provides a clear definition for the operationalization of ``online harassment'' as a superset, others appear to define the superset by the behaviors it encompasses (or, viewed another way, do not define the superset at all).  For example, in the Pew Research Center's 2014 report no online harassment, Duggan does not explicitly define harassment itself \cite{Duggan2014Online}.  Instead, Duggan refers to harassment as a spectrum that contains a range of (presumably negative) behaviors ``from garden-variety name calling to more threatening behavior'' [p. 1].  The specific harassing behaviors referred to in the report include offensive name-calling, physical threats, sustained harassment over a period of time, sexual harassment, and stalking.  Reiterating the notion that these behaviors exist on a continuum of severity, Duggan explains that men endure more harassment than women, but women are disproportionately more likely to be victimized by the more severe types of harassment.

Like Duggan \cite{Duggan2014Online}, Lenhart and colleagues \cite{Lenhart2016Online} also operationalize ``online harassment'' as both a superset and as a continuum.  They provide one of the most explicitly inclusive definitions of online harassment that we have seen:

\begin{displayquote}
Harassment can encompass a wide range of unwanted contact that is used to create an intimidating, annoying, frightening, or even hostile environment for the victim. Online harassment is generally recognized as referring to this type of negative and unwanted contact using digital means. Online harassment can be a brief occurrence or a sustained campaign of abuse and attacks; the perpetrator (or perpetrators) might be intimately known to the victim, or a stranger in another state or country. Online harassment is defined less by the specific behavior than its intended effect on and the way it is experienced by its target.
\end{displayquote}

They note that their operationalization differs from legal definitions, which require harassment to be methodical and/or repetitive in nature.  We would like to draw attention to the distinction at the end of their definition; to Lenhart and colleagues, whether or not a behavior constitutes online harassment is linked to the \textit{inent} behind the behavior.  By this rationale, it is unclear how one would classify behaviors by individuals who intend no harm to others, but which cause harm nonetheless.  Turkle notes that, perhaps due to a stark decrease in empathy over several decades, there are instances in which adolescents may hurt their friends over CMC platforms with no recognition or understanding of how their behaviors were are hurtful \cite{Turkle2015Reclaiming}.  

In addition to providing several fairly specific harassing behaviors (n=20, including the six behaviors reported on by \cite{Duggan2014Online}), Lenhart and colleagues \cite{Lenhart2016Online} create a slightly more complex taxonomy by including three intermediate categories.  Their 20 specific types of harassment fit into the broad categories of \textit{direct harassment}, \textit{invasion of privacy}, and \textit{denial of access}.  While these three categories seem useful for discussing harassment, it remains to be seen whether they will have utility for machine-learning approaches to stemming the behaviors contained within.

Lenhart and colleagues' \cite{Lenhart2016Online} definition of harassment is clearly stated and comprises specific behaviors which can be grouped together into categories, which may be convenient for the sake of discussion.  However, the report also includes some rather ambiguous language.  For example, early in the paper, Lenhart and colleagues define harassment and abuse as both consisting of ``unwanted contact that is used to create an intimidating, annoying, frightening, or even hostile environment for the victim and that uses digital means to reach the target'' [p. 3].  Being that the two terms appear to be synonymous, their use in the report's title, \textit{Online Harassment, Digital Abuse, and Cyberstalking in America} is confusing.  The title seems to indicate that online harassment, digital abuse, and cyberstalking are three different, but related, things.  Throughout the paper, the authors refer to both ``harassment and abuse'' and ``harassment or abuse,'' such that it is further unclear whether they intend for these terms to be used interchangeably, or whether there is some subtle, yet unstated, difference.  Additionally unclear is the rationale for the elevation of \textit{cyberstalking} to a separate entry in the paper's title, as it is referred to as a type of harassment (not something distinct from harassment and/or abuse) throughout the paper itself \footnotemark[1].  The comprehensiveness of Lenhart and colleagues' reported results, which are quite elucidating and build upon those of Duggan \cite{Duggan2014Online}, is tampered by the ambiguity injected by the loose usage of these terms.

\footnotetext[1]{In addition to being used in a somewhat confusing manner, there is a question of whether or not \textit{cyberstalking}, a which seems to imply something set apart from traditional stalking, is a useful term.  Some research shows that cyberstalking is much more of an outgrowth of traditional stalking, rather than a separate entity , and referring to it as something separate may obfuscate the severity of incidents by directing attention away from whatever stalking may simultaneously be happening to a victim offline \cite{Sheridan2007Is}.}

In contrast to Duggan \cite{Duggan2014Online}, Lenhart and colleagues \cite{Lenhart2016Online}, and Lenhart \cite{Lenhart2010Cyberbullying} who consider cyberbullying to be a type of online harassment, as well as in contrast to Lenhart \cite{Lenhart2007Cyberbullying} and Jones and colleagues \cite{Jones2013Online} who use the two terms interchangeably, Wolak and colleagues \cite{Wolak2007Does} ask whether online harassment is a manifestation of cyberbullying.  That is, whereas online harassment is typically operationalized as the superset into which cyberbullying fits, Wolak and colleagues wonder if the reverse might be true.  We will delve into their results and into their usage of the term ``cyberbullying'' later.  They defined online harassment as any threats or offensive behaviors, non-inclusive of sexual solicitation, sent to or posted publicly about a child or adolescent.  Implied in this definition, although not as clearly made explicit as in the previous examples, is a taxonomy of behaviors in which ``harassment'' is the umbrella term.  This operationalization is quite inclusive, albeit not as clear as that of Lenhart and colleagues \cite{Lenhart2016Online}, and only inclusive of behaviors targeted towards minors.  While the paper and its venue of publication are clearly focused on harassment as it pertains to minors, this particular definition ignores the fact that adults are also quite likely to be victims of online harassment \cite{Duggan2014Online,Lenhart2016Online}, making the definition potentially misleading.  Also interesting is the exlusion of sexual behaviors from their definition.  Not only are these included in other already stated definitions, they appear to be the category of behavior about which parents are most concerned \cite{Davis2015Parents}.

The examples discussed above are indicative of the ways in which researchers of the nature and reach of hurtful online behaviors talk about these phenomena.  Online harassment is generally a superset under which a myriad of behaviors fall.  Sometimes the behaviors are clustered into groups, and sometimes they're ranked by severity.  As researchers focus in on ever more specific types of harassment, definitions become more precise (and arguably, useful for discussing the behaviors themselves).  Just as Pater has found that different social-media platforms consider different behaviors to count as harassment \cite{Pater2016Characterizations}, so, too, do researchers.  Some researchers focus on singular, specific behaviors that comprise harassment \cite{Moor2010Flaming}.  There is, perhaps, a need for more of such papers to tell us about the more granular details of what online harassment looks like.  Interestingly, we found only a few instances in which self-harm was categorized as harassing behavior.  One of these instances was Pater \cite{Pater2016Characterizations}, who includes self-harm in her investigation of social media platforms' harassment policies.  The other was Boyd \cite{Boyd2014Bullying}, who notes that some degree of online harassment is perpetrated by adolescents against themselves, perhaps for attention or validation.

While research geared toward understanding harassment uses the vocabulary described above, research geared toward technical solutions to the problem of harassment has a different lexicon.  These studies often use terms other than online harassment.  When online harassment \textit{is} used, it is rarely operationalized at all.  Instead, it refers to a set of features of online content presumed to be undesirable.  For example, Dadvar and Jong \cite{Dadvar2012Cyberbullying} talk only about detecting ``harassing sentences.''  What are harassing sentences?  In this case, a sentence is harassing depending on whether it contains profanity, the pronouns employed, and the gender of the person that posted the content.  They use cyberbullying interchangeably.  This approach, in contrast to the definitions described earlier, is highly exclusionary and is likely to lead to a lot of mis-labeling of content \cite{Guberman2017Challenges}.  In another early case of applying machine-learning to online harassment, Yin and colleagues \cite{yin_detection_2009} first acknowledge the ambiguity surrounding the term ``online harassment'' before defining it as any intentional action meant to annoy another user in the community.  Beyond that, they zero-in on a specific class of harassing behavior ``in which a user systematically deprecates the contributions of another user'' [p. 2].  This definition is inclusive, and the specific behavior is on which Herring \cite{Herring1999Rhetorical} also identified.  While the definition they use is reasonable, the way it is operationalized in their classifiers is potentially problematic.  While they include contextual and sentiment features, the reliance on profanity is an unreliable indicator of harassment (as they themselves reported).  

\section{Cyber Bullying}

There appears to be a good deal of overlap between the ways in which researchers employ the terms ``online harassment'' and ``cyberbullying.''  In fact, there are researchers who create the exact types of taxonomies described in the previous section referring to the superset as ``cyberbullying'' rather than as ``online harassment'' (or use the two terms completely interchangeably) \cite{Lenhart2007Cyberbullying}.  More frequently, however, at least in the space in which researchers are focusing on the nature, reach, and impact of cyberbullying, definitions of the term tend to be less inclusive than those used in similar situations to describe online harassment.  The definitions of cyberbullying are more numerous and more varied.  In one case, in introducing the idea of cyberbullying, a single paper draws upon more than five separate definitions.  They consider bullying to be the use of ICTs to maliciously and repeatedly threaten people, threats sent via ICTs that cause psychological and social problems for victims, a type of psychological bullying occurring via ICTs, a form of social aggression, \textit{and} an extension of traditional bullying with several notable exceptions \cite{Cetin2011Cyber}.  Unfortunately, not all of the definitions the authors combine are compatible with one another.  In this section, we will discuss some of the myriad definitions of cyberbullying, definitions of cyberbullying based on traditional bullying and the problems thereof, and the ways in which different research communities are talking about cyberbullying in quite different ways.  Among those trying to figure out just what cyberbullying is, not including the definitions that are synonymous with the taxonomies of online harassment, two types definitions of cyberbullying prevail.

The dominant perspective on cyberbullying is that it is analagous to traditional bullying.  At the time of writing, when the authors Google ``what is cyberbullying?,'' the first hit is \href{https://www.stopbullying.gov/cyberbullying/what-is-it/index.html}{stopbullying.gov} \cite{u.s._department_of_health_and_human_services_what_2012}, which is maintained by the U.S. Department of Health and Human Services (HHS).  They define cyberbullying as ``bullying that takes place using electronic technology,'' which is inclusiuve of most ICTs.  To describe the nature of bullying, HHS defers to the widely accepted definition of Olweus \cite{Olweus1993Bullying}.  According to Olweus, bullying comprises incidents in which a child or adolescent is the victim of intentionally hurtful actions perpetrated by one or more other children or adolescents repeatedly, over a period of time.  For Olweus, in addition to being aggressive, bullying entails repetition over a period of time and an imbalance of power between the victim and the perpetrator(s).  In traditional bullying, this imbalance of power typically deals with age, strength, and other factors related to one's physical presence.  Olweus is very clear that, in situations in which the victim and the perpetrator are of similar physical or psychological strength, the term \textit{bullying} does not apply.  In highlighting repetition, Olweus acknowledges that there are certain types of one-time harassing behaviors that may, in fact, be for severe than typical repeated bullying behaviors.  However, he emphasizes repetition ``to exclude occasional nonserious negative actions that are directed against one student at one time or against another on a different occasion'' [p. 10].  Proponents of Olweus' definition in the cyberbullying research arena simply extend the traditional definition, including the three key features, to interactions occuring over one or more of the many CMC tools available today (e.g. \cite{Smith2008Cyberbullying}).  However, it's not immediately clear whether or not the traditional definition actually translates to CMC settings so easily.  Further, it remains uncertain whether the distinction between thusly defined cyberbullying behaviors and other hurtful behaviors online is important or useful.  As we will discuss below, it appears that some of the potentially harmful online behaviors most concerning to parents do not fit easily into this conception of cyberbullying.  What's more, it seems that this model of cyberbullying does not match the ways in which reserachers in other areas (i.e., those working on automated detection and intervention) and members of the general public think and talk about cyberbullying.

Only recently have some researchers begun to evaluate the applicability and utility of this particular conception of cyberbullying.  Generally, those researchers asking these questions agree that Olweus' imbalance of power criterion is satisfied by the the ubiquitous and often anonymous nature of CMC tools, which makes can make it difficult for potential victims to avoid potential bullies \cite{Grigg2010CyberAggression,Vandebosch2008Defining}.  As such, this particular facet of bullying appears to have a good analogue in online settings.  Things are less clear concerning the Olweus' requirement that bullying behaviors be repeated over a period of time.  Grigg \cite{Grigg2010CyberAggression} provides some examples of behaviors that don't appear to conform to Olweus' standard of repetition (e.g. outing, flaming and happy slapping\footnotemark[2]).  The desire to make cyberbullying as inclusive as possible while still conforming to the traditional definition seems to lead to interesting mental gymnastics.  Some researchers \cite{Dooley2009Cyberbullying,Grigg2010CyberAggression,Grigg2012Definitional} do their best to frame single, one-off harmful actions as being repetetive in nature.  Dooley and colleauges \cite{Dooley2009Cyberbullying} explain that a single act does not necesserily need to be repeated in order to continue to inflict harm on the intended victim.  As an example, they describe situations in which a perpetrator posts a very embarrassing online video about a victim.  The video is posted only once.  However, it is viewed by many people.  Dooley and colleagues consider the fact that the video is viewed by many people as a repetition of the original hurtful act.  Implicit here, and what is made explict by Grigg \cite{Grigg2012Definitional}, is that in this operationalization of repetition, the intent behind the original action becomes irrelevant.  Those who view the video are thus considered perpetrators, although they were not the individuals who recorded and/or posted the content with the intent to inflict harm upon the victim.  Certainly, it makes sense that the pain caused by this kind of online harassment can be more severe than that which is caused by traditional bullying by virtue of the wide reach and vast viewing audience of the internet.  However, Olweus \cite{Olweus1993Bullying} is quite clear about both repetition and intent, and some may find it difficult to justify placing these one-time video-posting type behaviors into Olweus' framework.  This video-posting behavior has physical-world analogs.  Consider an incident at a high-school in which a Senior posted a very emberassing picture of a Freshman on the bulliten board by the school's main entrance.  Would you consider this a singular act of harassment, or would the number of acts of harassment increase for every person who walked by the bulliten board before the picture was removed?  If you would consider the physical-world example to be a single act, and therefore not bullying by Olweus' standards, than a very similar online situation shouldn't be cyberbullying by the exact same standards and the attempt to render it such is reaching.  

Beyond the issue of whether or not the accepted definition of traditional bullying has direct online equivalents is the matter of whether or not such a conception of bullying is even \textit{useful} in online spaces.  As we've already touched on, this definition of cyberbullying excludes a wide variety of hurtful online behaviors.  Duggan \cite{Duggan2014Online} find that while 73\% of adult internet users have been victims of online harassment, only 7\% report being harassed online over a period of time.  It's unclear whether the proportion of youths who experience repeated victimization online is higher than that of adults.  However, Wolak and colleagues' findings \cite{Wolak2007Does} suggest that, among youths, repetition is likely only in cases where the online incidents are an outgrowth or continuation of events also taking place offline. In light of what we know about the amount of repated harassment, relative to all harassment, it becomes even more clear that this usage cyberbullying misses the mark.  Indeed, a several fairly recent papers reach this conclusion.  When Grigg \cite{Grigg2010CyberAggression} asked laypeople to describe cyberbullying, they tended to consider nearly all negative online behaviors as cyberbullying.  In light of the official definition, Grigg's participants found the term to be ``vague, inadequte, and restrict[ive'' [p. 151].  Under this definition, too many behaviors don't count or aren't punishable.  Grigg, instead, suggests we operationalize negative online behavior in terms of aggression, which we will discuss in a later section.  In a follow up study (which needs to be replicated with a larger sample, and possibly without priming subjects), Grigg concludes that, based on the poor fit of the traditional definiton of bullying and the public's perception of what is and is not cyberbullying, cyberbullying should be viewed as a distinct phenomena - not as an extension of traditional bullying \cite{Grigg2012Definitional}.  


\footnotetext[2]{see Smith \cite{Smith2008Cyberbullying}}


\section{Cyber Aggression}

\section{Toxicity}

\section{Moving Forward}

% BALANCE COLUMNS
\balance{}

% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{Remote.bib}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
